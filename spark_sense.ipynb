{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvZLJaOaMK43k0/qic/LfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitornimschofsky/spark/blob/main/spark_sense.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARTE 0**\n",
        "\n",
        "**CONFIGURANDO O SPARK NO AMBIENTE DO COLAB**"
      ],
      "metadata": {
        "id": "22ez9cYcMzkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZuyWt8Z-Mfk6"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n"
      ],
      "metadata": {
        "id": "yHlX_TqMNBON"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "mhbkSs2bNEob"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARTE 1**\n",
        "\n",
        "**GERANDO O DF**"
      ],
      "metadata": {
        "id": "PCGEGZfMNqrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"PySparkTest\").getOrCreate()\n",
        "\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", 34, \"Data Scientist\"),\n",
        "    (\"Bob\", 45, \"Data Engineer\"),\n",
        "    (\"Cathy\", 29, \"Data Analyst\"),\n",
        "    (\"David\", 35, \"Data Scientist\")\n",
        "]\n",
        "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nn3PzYzNLDW",
        "outputId": "836008cd-0bec-4046-c965-54bdfb5f42ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------------+\n",
            "| Name|Age|    Occupation|\n",
            "+-----+---+--------------+\n",
            "|Alice| 34|Data Scientist|\n",
            "|  Bob| 45| Data Engineer|\n",
            "|Cathy| 29|  Data Analyst|\n",
            "|David| 35|Data Scientist|\n",
            "+-----+---+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOME E IDADE DE QUEM TEM MAIS DE 30 ANOS**"
      ],
      "metadata": {
        "id": "jxDEh3AJOBIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected = df.select(\"Name\", \"Age\")\n",
        "\n",
        "df_filtered = df_selected.filter(df_selected[\"Age\"] > 30)\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmeFR4xmN0g7",
        "outputId": "b199e5cd-a33e-405b-8ae5-609ca6c8d264"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 34|\n",
            "|  Bob| 45|\n",
            "|David| 35|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CALCULANDO A MÉDIA DE IDADE POR OCUPAÇÃO**"
      ],
      "metadata": {
        "id": "emHVcZPPOcj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "df_grouped = df.groupBy(\"Occupation\").agg(avg(\"Age\").alias(\"Average_Age\"))\n",
        "df_grouped.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuaaejrsN_UI",
        "outputId": "74d72ea3-3455-4aa7-c6d9-147f38084eb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+\n",
            "|    Occupation|Average_Age|\n",
            "+--------------+-----------+\n",
            "|Data Scientist|       34.5|\n",
            "|  Data Analyst|       29.0|\n",
            "| Data Engineer|       45.0|\n",
            "+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ORDENANDO PELA MÉDIA DE IDADE EM ORDEM DESC**"
      ],
      "metadata": {
        "id": "NrP33ZphOqtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = df_grouped.orderBy(\"Average_Age\", ascending=False)\n",
        "df_sorted.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dePEnPIOkF_",
        "outputId": "bce85ad9-ab7f-4d0b-ea8d-7b4948a4671b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+\n",
            "|    Occupation|Average_Age|\n",
            "+--------------+-----------+\n",
            "| Data Engineer|       45.0|\n",
            "|Data Scientist|       34.5|\n",
            "|  Data Analyst|       29.0|\n",
            "+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARTE 2**\n",
        "\n",
        "**UDFs (User Defined Functions)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p5ao5-WaPIEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def categorize_age(age):\n",
        "    if age < 30:\n",
        "        return \"Jovem\"\n",
        "    elif 30 <= age <= 40:\n",
        "        return \"Adulto\"\n",
        "    else:\n",
        "        return \"Senior\"\n",
        "\n",
        "categorize_age_udf = udf(categorize_age, StringType())\n",
        "\n",
        "df_with_category = df.withColumn(\"Age_Category\", categorize_age_udf(df[\"Age\"]))\n",
        "df_with_category.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpXeTCidPOWB",
        "outputId": "bd951bed-b263-4f52-a2b4-c4c219d8eb33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------------+------------+\n",
            "| Name|Age|    Occupation|Age_Category|\n",
            "+-----+---+--------------+------------+\n",
            "|Alice| 34|Data Scientist|      Adulto|\n",
            "|  Bob| 45| Data Engineer|      Senior|\n",
            "|Cathy| 29|  Data Analyst|       Jovem|\n",
            "|David| 35|Data Scientist|      Adulto|\n",
            "+-----+---+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FUNÇÃO DE JANELA** ***(Não sei se entendi corretamente esse calculo, levei em consideração a criação de apenas uma coluna conforme descrição da questão de janelas.)\n"
      ],
      "metadata": {
        "id": "2mD8-d8wQNbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "window_spec = Window.partitionBy(\"Occupation\")\n",
        "\n",
        "df_with_diff = (df.withColumn(\"Avg\", avg(\"Age\").over(window_spec))\n",
        "                  .withColumn(\"Age_Diff_From_Avg\", df[\"Age\"] - avg(\"Age\").over(window_spec))\n",
        ")\n",
        "df_with_diff.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYV4PTM9QSFL",
        "outputId": "fc73ccee-a3fd-4a0b-dfaa-f50df65ca205"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+--------------+----+-----------------+\n",
            "| Name|Age|    Occupation| Avg|Age_Diff_From_Avg|\n",
            "+-----+---+--------------+----+-----------------+\n",
            "|Alice| 34|Data Scientist|34.5|             -0.5|\n",
            "|David| 35|Data Scientist|34.5|              0.5|\n",
            "|Cathy| 29|  Data Analyst|29.0|              0.0|\n",
            "|  Bob| 45| Data Engineer|45.0|              0.0|\n",
            "+-----+---+--------------+----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARTE 3**\n",
        "\n",
        "**PARTICIONAMENTE E OTIMIZAÇÃO**"
      ],
      "metadata": {
        "id": "MSkLZZleXN4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Particionamento é uma estratégia usada para dividir um conjunto de dados em várias partes menores, chamadas de partições, com base em uma coluna específica. No PySpark, essa abordagem pode acelerar as operações de leitura e escrita, pois o Spark consegue processar as partições simultaneamente. Isso é especialmente vantajoso para grandes volumes de dados, onde as operações de entrada e saída (I/O) podem se tornar lentas. Ao particionar de forma eficaz, o Spark acessa apenas as partições necessárias para uma operação, evitando a leitura de dados desnecessários e melhorando o desempenho geral."
      ],
      "metadata": {
        "id": "JDH2MP-AXW0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exemplo de particionamento de um dataFrame por uma coluna específica (\"Date\")\n",
        "df_partitioned = df.repartition(\"Date\")\n",
        "\n",
        "df_partitioned.write.partitionBy(\"Date\").parquet(\"/path/to/output/dir\")"
      ],
      "metadata": {
        "id": "QAlgfuM0XYvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BROADCAST JOIN**"
      ],
      "metadata": {
        "id": "akOEWL6WXlUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Broadcast Join é uma técnica no PySpark que melhora o desempenho das operações de join, especialmente quando um dos DataFrames é pequeno o bastante para ser armazenado na memória. Em vez de fazer um join distribuído, que pode consumir muito tempo e recursos, o Spark envia (ou \"broadcast\") o DataFrame menor para todos os nós do cluster. Assim, cada nó pode realizar o join de forma local, evitando a redistribuição de dados entre os nós."
      ],
      "metadata": {
        "id": "pAtl3SscX59S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "\n",
        "df_large = spark.read.parquet(\"/path/to/large_dataset\")\n",
        "df_small = spark.read.parquet(\"/path/to/small_dataset\")\n",
        "\n",
        "\n",
        "df_joined = df_large.join(broadcast(df_small), \"key_column\")\n",
        "\n",
        "df_joined.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nX5pSvmYX6st",
        "outputId": "798c0591-63bd-4926-8267-28e61c6623a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Path does not exist: file:/path/to/large_dataset;",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0b66b442c669>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_large\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/large_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_small\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/small_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    351\u001b[0m         self._set_opts(mergeSchema=mergeSchema, pathGlobFilter=pathGlobFilter,\n\u001b[1;32m    352\u001b[0m                        recursiveFileLookup=recursiveFileLookup)\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/path/to/large_dataset;"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARTE 4**\n",
        "\n",
        "**INTEGRAÇÃO COM OUTRAS TECNOLOGIAS**"
      ],
      "metadata": {
        "id": "1PlARr_lYIuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# leitura arquivo CSV\n",
        "df = spark.read.csv(\"/path/to/input.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# salvar o dataFrame em formato parquet\n",
        "df.write.parquet(\"/path/to/output.parquet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "0UYZohQrYLBY",
        "outputId": "0069b9fe-fae8-4ad3-a14c-2ca75a3be871"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Path does not exist: file:/path/to/input.csv;",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2b12f9aeae15>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# leitura arquivo CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/input.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# salvar o dataFrame em formato parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/output.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/path/to/input.csv;"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTEGRAÇÃO COM O HADOOP**"
      ],
      "metadata": {
        "id": "TMjByFDjYon_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "PySpark se integra facilmente com o Hadoop HDFS (Hadoop Distributed File System) para leitura e escrita de dados. O Spark pode ler e escrever arquivos diretamente no HDFS, permitindo o processamento de grandes volumes de dados distribuídos. Isso é feito através do uso do URI do HDFS ao especificar o caminho do arquivo."
      ],
      "metadata": {
        "id": "G_2Klsz_Ylwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# leitura de um arquivo do HDFS\n",
        "df = spark.read.csv(\"hdfs://namenode:9000/path/to/input.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# processamento do dataFrame (apens um exemplo)\n",
        "df_filtered = df.filter(df[\"column\"] > 100)\n",
        "\n",
        "# escrita do resultado de volta no HDFS\n",
        "df_filtered.write.csv(\"hdfs://namenode:9000/path/to/output.csv\")\n"
      ],
      "metadata": {
        "id": "g-UVUw7uY-aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARTE 5**\n",
        "\n",
        "**LOG**"
      ],
      "metadata": {
        "id": "PI-39JAwZRAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carregar o arquivo de log em um DataFrame**"
      ],
      "metadata": {
        "id": "3ERCJD6JdCAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_logs = spark.read.csv(\"/path/to/log  .csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "dmn4igiuc9hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contar o número de ações realizadas por cada usuário**"
      ],
      "metadata": {
        "id": "HqF0DAHJdG8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_user_actions = df_logs.groupBy(\"user_id\").count()\n"
      ],
      "metadata": {
        "id": "nHO1swBFdJDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encontrar os 10 usuários mais ativos**"
      ],
      "metadata": {
        "id": "HlFdd551dKVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_users = df_user_actions.orderBy(\"count\", ascending=False).limit(10)\n",
        "df_top_users.show()\n"
      ],
      "metadata": {
        "id": "brdFp0hidMgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Salvar o resultado em um arquivo CSV**"
      ],
      "metadata": {
        "id": "gakwoOxmdODX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_top_users.write.csv(\"/path/to/output/top_users.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "hyPi85Q_dTgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}